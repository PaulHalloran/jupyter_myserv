{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris\n",
    "import iris.coord_categorisation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import iris.quickplot as qplt\n",
    "import netCDF4\n",
    "import datetime\n",
    "import scipy\n",
    "import scipy.signal\n",
    "import glob\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from scipy.stats import t\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Potential issues to think about:\n",
    "    William uses: Might only SST values (because from satilite). Models are at best deaily mean\n",
    "    They use a standardising year of 1988.3 which would not necessarily be a sensble year to use given model metholology\n",
    "    \n",
    "\n",
    "\n",
    "Skirving methodology\n",
    "Regarding the current methodology for deriving the MMM it goes like this:\n",
    "\n",
    "\n",
    "\n",
    "1)  for the period of 1985 to 2012, turn each yearly month into an average SST value (only night-only SST values).\n",
    "2)  for each month, fit a linear regression to the 28 SST values across all years  ie Y = a + bX, where Y is the monthly averaged SST and X is the year\n",
    "3)  at this stage you will have 12 linear regressions per pixel, one for each month.  Determine the SST value for X = 1988.3\n",
    "4)  this will provide you with 12 SST values, the maximum of which will be the MMM value for that pixel, the month that this value represents is called the MMM month.\n",
    "\"\"\"\n",
    "\n",
    "directory = '/data/dataSSD1/ph290/three_hourly/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_region(cube,lon_west,lon_east,lat_south,lat_north):\n",
    "    cube_region_tmp = cube.intersection(longitude=(lon_west, lon_east))\n",
    "    cube_region = cube_region_tmp.intersection(latitude=(lat_south, lat_north))\n",
    "    return cube_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_avg(cube):\n",
    "    try:\n",
    "        cube.coord('latitude').guess_bounds()\n",
    "        cube.coord('longitude').guess_bounds()\n",
    "    except:\n",
    "        pass\n",
    "    grid_areas = iris.analysis.cartography.area_weights(cube)\n",
    "    return cube.collapsed(['longitude','latitude'],iris.analysis.MEAN, weights=grid_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linregress_3D(y_array):\n",
    "    # y_array is a 3-D array formatted like (time,lon,lat)\n",
    "    # The purpose of this function is to do linear regression using time series of data over each (lon,lat) grid box with consideration of ignoring np.nan\n",
    "    # Construct x_array indicating time indexes of y_array, namely the independent variable.\n",
    "    x_array=np.empty(y_array.shape)\n",
    "    for i in range(y_array.shape[0]): x_array[i,:,:]=i+1 # This would be fine if time series is not too long. Or we can use i+yr (e.g. 2019).\n",
    "    x_array[np.isnan(y_array)]=np.nan\n",
    "    # Compute the number of non-nan over each (lon,lat) grid box.\n",
    "    n=np.sum(~np.isnan(x_array),axis=0)\n",
    "    # Compute mean and standard deviation of time series of x_array and y_array over each (lon,lat) grid box.\n",
    "    x_mean=np.nanmean(x_array,axis=0)\n",
    "    y_mean=np.nanmean(y_array,axis=0)\n",
    "    x_std=np.nanstd(x_array,axis=0)\n",
    "    y_std=np.nanstd(y_array,axis=0)\n",
    "    # Compute co-variance between time series of x_array and y_array over each (lon,lat) grid box.\n",
    "    cov=np.nansum((x_array-x_mean)*(y_array-y_mean),axis=0)/n\n",
    "    # Compute correlation coefficients between time series of x_array and y_array over each (lon,lat) grid box.\n",
    "    cor=cov/(x_std*y_std)\n",
    "    # Compute slope between time series of x_array and y_array over each (lon,lat) grid box.\n",
    "    slope=cov/(x_std**2)\n",
    "    # Compute intercept between time series of x_array and y_array over each (lon,lat) grid box.\n",
    "    intercept=y_mean-x_mean*slope\n",
    "    # Compute tstats, stderr, and p_val between time series of x_array and y_array over each (lon,lat) grid box.\n",
    "    tstats=cor*np.sqrt(n-2)/np.sqrt(1-cor**2)\n",
    "    stderr=slope/tstats\n",
    "    p_val=t.sf(tstats,n-2)*2\n",
    "    # Compute r_square and rmse between time series of x_array and y_array over each (lon,lat) grid box.\n",
    "    # r_square also equals to cor**2 in 1-variable lineare regression analysis, which can be used for checking.\n",
    "    r_square=np.nansum((slope*x_array+intercept-y_mean)**2,axis=0)/np.nansum((y_array-y_mean)**2,axis=0)\n",
    "    rmse=np.sqrt(np.nansum((y_array-slope*x_array-intercept)**2,axis=0)/n)\n",
    "    # Do further filteration if needed (e.g. We stipulate at least 3 data records are needed to do regression analysis) and return values\n",
    "    n=n*1.0 # convert n from integer to float to enable later use of np.nan\n",
    "    n[n<3]=np.nan\n",
    "    slope[np.isnan(n)]=np.nan\n",
    "    intercept[np.isnan(n)]=np.nan\n",
    "    p_val[np.isnan(n)]=np.nan\n",
    "    r_square[np.isnan(n)]=np.nan\n",
    "    rmse[np.isnan(n)]=np.nan\n",
    "#     return n,slope,intercept,p_val,r_square,rmse\n",
    "    return slope,intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/joc.3486\n",
    "# the monthly data set for 1982 â€“ 2006 was first detrended using a linear regression, calculated for each month of the year and grid cell. The data set was detrended and centred on 1988\n",
    "# I don't like this - it becoes ver deending on whether 1988 is a warm or cold year/...\n",
    "\n",
    "def mmm(cube,years_for_mmm_climatology):\n",
    "    cube_years = cube.coord('year').points\n",
    "    #subset the data into the bit you want to use to calculate the MMM climatology and the bit you want to calculate DHW on\n",
    "    clim_cube = cube[np.where((cube_years > years_for_mmm_climatology[0]) & (cube_years < years_for_mmm_climatology[1]))]\n",
    "    #collapse the months together, taking the maximum value at each lat-lon grid square\n",
    "    mmm_climatology = clim_cube.collapsed('time',iris.analysis.MAX)\n",
    "    return mmm_climatology\n",
    "\n",
    "def mmm_with_detrending(cube,years_for_mmm_climatology):\n",
    "    cube_years = cube.coord('year').points\n",
    "    #subset the data into the bit you want to use to calculate the MMM climatology and the bit you want to calculate DHW on\n",
    "    clim_cube = cube[np.where((cube_years > years_for_mmm_climatology[0]) & (cube_years < years_for_mmm_climatology[1]))]\n",
    "    monthly_mean_climatology = clim_cube.collapsed('time',iris.analysis.MEAN)\n",
    "    clim_cube_detrended = clim_cube.copy()\n",
    "    clim_cube_detrended.data = scipy.signal.detrend(clim_cube_detrended.data,axis = 0)\n",
    "    clim_cube_final = clim_cube_detrended + monthly_mean_climatology\n",
    "    #collapse the months together, taking the maximum value at each lat-lon grid square\n",
    "    mmm_climatology = clim_cube_final.collapsed('time',iris.analysis.MAX)\n",
    "    return mmm_climatology\n",
    "\n",
    "\n",
    "def mmm_skirving(cube):\n",
    "    print 'NOTE THIS SHOULD BE USING NIGHT TIME TEMPERATURES'\n",
    "    years_for_mmm_climatology = [1985,2012]\n",
    "    standardisation_date = 1988.3\n",
    "    mm_cube = cube[0:12].copy()\n",
    "    mm_cube_data = mm_cube.data.copy()\n",
    "    cube_years = cube.coord('year').points\n",
    "    #subset the data into the bit you want to use to calculate the MMM climatology and the bit you want to calculate DHW on\n",
    "    clim_cube = cube[np.where((cube_years >= years_for_mmm_climatology[0]) & (cube_years <= years_for_mmm_climatology[1]))]\n",
    "    clim_cube_detrended = clim_cube.copy()\n",
    "    clim_cube_detrended_data = clim_cube_detrended.data\n",
    "    print np.shape(clim_cube_detrended)\n",
    "    for i,month in enumerate(np.unique(cube.coord('month_number').points)):\n",
    "        loc = np.where(clim_cube.coord('month_number').points == month)\n",
    "        tmp = clim_cube_detrended_data[loc,:,:].data[0]\n",
    "        tmp[np.where(tmp > 1.0e19 )] = np.nan\n",
    "        slope,intercept = linregress_3D(tmp)\n",
    "        x = standardisation_date - years_for_mmm_climatology[0]\n",
    "        y = (slope * x ) + intercept\n",
    "        mm_cube_data[i,:,:] = y\n",
    "    mm_cube.data = mm_cube_data\n",
    "    mmm_climatology = mm_cube.collapsed('time',iris.analysis.MAX)\n",
    "    return mmm_climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dhm(cube,mmm_climatology,years_over_which_to_calculate_dhw):\n",
    "    cube_years = cube.coord('year').points\n",
    "    main_cube = cube[np.where((cube_years > years_over_which_to_calculate_dhw[0]) & (cube_years < years_over_which_to_calculate_dhw[1]))]\n",
    "    #subtract the monthly mean climatology from the rest of the data\n",
    "    main_cube -= mmm_climatology\n",
    "\n",
    "    #set all values less than 1 to zero\n",
    "    main_cube.data[np.where(main_cube.data <= 1.0)] = 0.0\n",
    "    #OR\n",
    "#     main_cube.data[np.where(main_cube.data <= 0.0)] = 0.0\n",
    "\n",
    "    #make a cube to hold the output data\n",
    "    output_cube = main_cube[2::].copy()\n",
    "    output_cube.data[:] = np.nan\n",
    "    output_cube_data = output_cube.data.copy()\n",
    "\n",
    "    #loop through from day 84 to the end of the dataset\n",
    "    for i in range(output_cube.shape[0]):\n",
    "#         print i,' of ',output_cube.shape[0]\n",
    "        #sum the temperatures in that 84 day window and divide result by 7 to get in DHWeeks rather than DHdays\n",
    "        tmp_data = main_cube[i:i+3].collapsed('time',iris.analysis.SUM)\n",
    "        output_cube_data[i,:,:] = tmp_data.data\n",
    "\n",
    "    #save the output\n",
    "    output_cube.data = output_cube_data\n",
    "    return output_cube\n",
    "\n",
    "\n",
    "\n",
    "def dhm_Spillman_2013(cube,mmm_climatology,years_over_which_to_calculate_dhw):\n",
    "    cube_years = cube.coord('year').points\n",
    "    main_cube = cube[np.where((cube_years > years_over_which_to_calculate_dhw[0]) & (cube_years < years_over_which_to_calculate_dhw[1]))]\n",
    "    #subtract the monthly mean climatology from the rest of the data\n",
    "    main_cube -= mmm_climatology\n",
    "\n",
    "    #set all values less than 1 to zero\n",
    "#     main_cube.data[np.where(main_cube.data <= 1.0)] = 0.0\n",
    "    #OR\n",
    "    main_cube.data[np.where(main_cube.data <= 0.0)] = 0.0\n",
    "\n",
    "    #make a cube to hold the output data\n",
    "    output_cube = main_cube[2::].copy()\n",
    "    output_cube.data[:] = np.nan\n",
    "    output_cube_data = output_cube.data.copy()\n",
    "\n",
    "    #loop through from day 84 to the end of the dataset\n",
    "    for i in range(output_cube.shape[0]):\n",
    "#         print i,' of ',output_cube.shape[0]\n",
    "        #sum the temperatures in that 84 day window and divide result by 7 to get in DHWeeks rather than DHdays\n",
    "        tmp_data = main_cube[i:i+3].collapsed('time',iris.analysis.SUM)\n",
    "        output_cube_data[i,:,:] = tmp_data.data\n",
    "\n",
    "    #save the output\n",
    "    output_cube.data = output_cube_data\n",
    "    return output_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dhw(cube,mmm_climatology,years_over_which_to_calculate_dhw):\n",
    "    cube_years = cube.coord('year').points\n",
    "    #note this is to be uef with daily data...\n",
    "    main_cube = cube[np.where((cube_years > years_over_which_to_calculate_dhw[0]) & (cube_years < years_over_which_to_calculate_dhw[1]))]\n",
    "    #subtract the monthly mean climatology from the rest of the data\n",
    "    main_cube -= mmm_climatology\n",
    "\n",
    "    #set all values less than 1 to zero\n",
    "    main_cube.data[np.where(main_cube.data <= 1.0)] = 0.0\n",
    "\n",
    "    #make a cube to hold the output data\n",
    "    output_cube = main_cube[83::].copy()\n",
    "    output_cube.data[:] = np.nan\n",
    "    output_cube_data = output_cube.data.copy()\n",
    "\n",
    "    #loop through from day 84 to the end of the dataset\n",
    "    for i in range(output_cube.shape[0]):\n",
    "#         print i,' of ',output_cube.shape[0]\n",
    "        #sum the temperatures in that 84 day window and divide result by 7 to get in DHWeeks rather than DHdays\n",
    "        tmp_data = main_cube[i:i+3].collapsed('time',iris.analysis.SUM)/7\n",
    "        output_cube_data[i,:,:] = tmp_data.data\n",
    "\n",
    "    #save the output\n",
    "    output_cube.data = output_cube_data\n",
    "    return output_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_two_cubes_extract_midnight(cubes):\n",
    "    #extract data for midnight\n",
    "    try:\n",
    "        iris.coord_categorisation.add_hour(cubes[0], 'time', name='hour')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        iris.coord_categorisation.add_hour(cubes[1], 'time', name='hour')\n",
    "    except:\n",
    "        pass\n",
    "    cube_tmp1 = cubes[0][np.where(cubes[0].coord('hour').points == 0)]\n",
    "    cube_tmp2 = cubes[1][np.where(cubes[1].coord('hour').points == 0)]\n",
    "                    \n",
    "    data1 = cube_tmp1.data\n",
    "    data2 = cube_tmp2.data\n",
    "    data = np.concatenate([data1,data2],axis=0)\n",
    "    data = np.ma.masked_where(data == cubes[0].data.fill_value, data)\n",
    "\n",
    "    length = cube_tmp1.shape[0] + cube_tmp2.shape[0]\n",
    "    datetime_object1 = netCDF4.num2date(cube_tmp1.coord('time').points,cube_tmp1.coord('time').units.name,cube_tmp1.coord('time').units.calendar)\n",
    "    datetime_object2 = netCDF4.num2date(cube_tmp2.coord('time').points,cube_tmp12.coord('time').units.name,cube_tmp2.coord('time').units.calendar)\n",
    "    datetime_object = np.concatenate([datetime_object1,datetime_object2])\n",
    "    try:\n",
    "        tmp =  [x._to_real_datetime() - datetime.datetime(1850,1,1) for x in datetime_object]\n",
    "    except:\n",
    "        tmp =  [x - datetime.datetime(1850,1,1) for x in datetime_object]\n",
    "    days_since_18500101 = [x.days for x in tmp]\n",
    "\n",
    "    time = iris.coords.DimCoord(days_since_18500101, standard_name='time',long_name=u'time', var_name='time', units='days since 1850-1-1')\n",
    "    latitude = iris.coords.DimCoord(range(-90, 90, 1), standard_name='latitude', units='degrees')\n",
    "    longitude = iris.coords.DimCoord(range(0, 360, 1), standard_name='longitude', units='degrees')\n",
    "    cube = iris.cube.Cube(data,standard_name='sea_surface_temperature',long_name='Sea Surface Temperature', var_name='tos', units='K',dim_coords_and_dims=[(time,0), (latitude, 1),\n",
    "    (longitude, 2)])\n",
    "    iris.coord_categorisation.add_year(cube, 'time', name='year')\n",
    "    iris.coord_categorisation.add_month(cube, 'time', name='month')\n",
    "    iris.coord_categorisation.add_month_number(cube, 'time', name='month_number')\n",
    "    return cube\n",
    "\n",
    "\n",
    "def merge_two_cubes_daily_average(cubes):\n",
    "    #calculate daily mean from 3 hour snapshots\n",
    "    try:\n",
    "        iris.coord_categorisation.add_day_of_year(cubes[0], 'time', name='day')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        iris.coord_categorisation.add_day_of_year(cubes[1], 'time', name='day')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        iris.coord_categorisation.add_year(cubes[0], 'time', name='year')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        iris.coord_categorisation.add_year(cubes[1], 'time', name='year')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    cubes[0] = cubes[0].aggregated_by(['year','day'], iris.analysis.MEAN)\n",
    "    cubes[1] = cubes[1].aggregated_by(['year','day'], iris.analysis.MEAN)\n",
    "    \n",
    "    data1 = cubes[0].data\n",
    "    data2 = cubes[1].data\n",
    "    data = np.concatenate([data1,data2],axis=0)\n",
    "    data = np.ma.masked_where(data == cubes[0].data.fill_value, data)\n",
    "\n",
    "    length = cubes[0].shape[0] + cubes[1].shape[0]\n",
    "    datetime_object1 = netCDF4.num2date(cubes[0].coord('time').points,cubes[0].coord('time').units.name,cubes[0].coord('time').units.calendar)\n",
    "    datetime_object2 = netCDF4.num2date(cubes[1].coord('time').points,cubes[1].coord('time').units.name,cubes[1].coord('time').units.calendar)\n",
    "    datetime_object = np.concatenate([datetime_object1,datetime_object2])\n",
    "    try:\n",
    "        tmp =  [x._to_real_datetime() - datetime.datetime(1850,1,1) for x in datetime_object]\n",
    "    except:\n",
    "        tmp =  [x - datetime.datetime(1850,1,1) for x in datetime_object]\n",
    "    days_since_18500101 = [x.days for x in tmp]\n",
    "\n",
    "    time = iris.coords.DimCoord(days_since_18500101, standard_name='time',long_name=u'time', var_name='time', units='days since 1850-1-1')\n",
    "    latitude = iris.coords.DimCoord(range(-90, 90, 1), standard_name='latitude', units='degrees')\n",
    "    longitude = iris.coords.DimCoord(range(0, 360, 1), standard_name='longitude', units='degrees')\n",
    "    cube = iris.cube.Cube(data,standard_name='sea_surface_temperature',long_name='Sea Surface Temperature', var_name='tos', units='K',dim_coords_and_dims=[(time,0), (latitude, 1),\n",
    "    (longitude, 2)])\n",
    "    iris.coord_categorisation.add_year(cube, 'time', name='year')\n",
    "    iris.coord_categorisation.add_month(cube, 'time', name='month')\n",
    "    iris.coord_categorisation.add_month_number(cube, 'time', name='month_number')\n",
    "    return cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_two_cubes_extract_midnight2(cube):\n",
    "    #extract data for midnight\n",
    "    try:\n",
    "        iris.coord_categorisation.add_hour(cube, 'time', name='hour')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    cube = cube[np.where(cube.coord('hour').points == 0)]\n",
    "                    \n",
    "    cube.data = np.ma.masked_where(cube.data == cube.data.fill_value, cube.data)\n",
    "\n",
    "    iris.coord_categorisation.add_month(cube, 'time', name='month')\n",
    "    iris.coord_categorisation.add_month_number(cube, 'time', name='month_number')\n",
    "    return cube\n",
    "\n",
    "\n",
    "def merge_two_cubes_daily_average2(cube):\n",
    "    #calculate daily mean from 3 hour snapshots\n",
    "    try:\n",
    "        iris.coord_categorisation.add_day_of_year(cube, 'time', name='day')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        iris.coord_categorisation.add_year(cube, 'time', name='year')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    cube = cube.aggregated_by(['year','day'], iris.analysis.MEAN)\n",
    "    \n",
    "    cube.data = np.ma.masked_where(cube.data == cube.data.fill_value, cube.data)\n",
    "\n",
    "    iris.coord_categorisation.add_month(cube, 'time', name='month')\n",
    "    iris.coord_categorisation.add_month_number(cube, 'time', name='month_number')\n",
    "    return cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up the dictonary with model names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['MPI-ESM1-2-HR']\n",
    "\n",
    "colors = ['r','g','b','k','m','c']\n",
    "model_dict = {}\n",
    "for i,model in enumerate(models):\n",
    "    model_dict[model] = {}\n",
    "    model_dict[model]['color'] = colors[i]\n",
    "\n",
    "\n",
    "# tos_3hr_MPI-ESM1-2-HR_historical_r1i1p1f1_gn_regridded.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read in the model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ph290/anaconda2/lib/python2.7/site-packages/iris/fileformats/cf.py:798: UserWarning: Missing CF-netCDF measure variable u'areacello', referenced by netCDF variable u'tos'\n",
      "  warnings.warn(message % (variable_name, nc_var_name))\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Failed to realise the lazy data as there was not enough memory available.\nThe data shape would have been (248368, 180, 360) with dtype('float32').\n Consider freeing up variables or indexing the data before trying again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-16c321522873>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mcubes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'tos_3hr_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_*r1i1p1f1_gn_regridded.nc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mcube2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_two_cubes_daily_average\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcubes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mcube_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cube_daily_avg'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcube2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-26b712c2d72b>\u001b[0m in \u001b[0;36mmerge_two_cubes_daily_average\u001b[0;34m(cubes)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mcubes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcubes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregated_by\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMEAN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mcubes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcubes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregated_by\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMEAN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mdata1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcubes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ph290/anaconda2/lib/python2.7/site-packages/iris/cube.pyc\u001b[0m in \u001b[0;36maggregated_by\u001b[0;34m(self, coords, aggregator, **kwargs)\u001b[0m\n\u001b[1;32m   3423\u001b[0m             \u001b[0;31m# data on first pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3424\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3425\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misMaskedArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3426\u001b[0m                     \u001b[0maggregateby_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3427\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ph290/anaconda2/lib/python2.7/site-packages/iris/cube.pyc\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m         \"\"\"\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ph290/anaconda2/lib/python2.7/site-packages/iris/_data_manager.pyc\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m                         \u001b[0;34m'Consider freeing up variables or indexing the data '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         'before trying again.')\n\u001b[0;32m--> 227\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mMemoryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;31m# Check the manager contract, as the managed data has changed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Failed to realise the lazy data as there was not enough memory available.\nThe data shape would have been (248368, 180, 360) with dtype('float32').\n Consider freeing up variables or indexing the data before trying again."
     ]
    }
   ],
   "source": [
    "test = os.path.isfile('/data/dataSSD1/ph290/three_hourly/dhw_test.pickle')\n",
    "\n",
    "if test:\n",
    "    in_pickle = open('/data/dataSSD1/ph290/three_hourly/dhw_test.pickle', 'r')\n",
    "    cube_dict = pickle.load(in_pickle)\n",
    "    in_pickle.close()\n",
    "else:\n",
    "    cube_dict = {}\n",
    "\n",
    "    for model in models:\n",
    "        cube_dict[model] = {}\n",
    "        # file = glob.glob(model+'_tos_*_r1i1p1f1_gn.nc')\n",
    "        # print file\n",
    "\n",
    "        cubes = iris.load_cube(directory+'tos_3hr_'+model+'_*r1i1p1f1_gn_regridded.nc')\n",
    "        cube= merge_two_cubes_extract_midnight2(cubes)\n",
    "        cube_dict[model]['cube_midnight'] = cube\n",
    "        \n",
    "        cubes = iris.load_cube(directory+'tos_3hr_'+model+'_*r1i1p1f1_gn_regridded.nc')\n",
    "        cube2 = merge_two_cubes_daily_average2(cubes)                                   \n",
    "        cube_dict[model]['cube_daily_avg'] = cube2\n",
    "    \n",
    "    out = open('/data/dataSSD1/ph290/three_hourly/dhw_test.pickle', 'w')\n",
    "    pickle.dump(cube_dict, out )\n",
    "    out.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-- -- -- ... -- -- --]\n",
      " [-- -- -- ... -- -- --]\n",
      " [-- -- -- ... -- -- --]\n",
      " ...\n",
      " [-1.899847388267517 -1.899835467338562 -1.899824857711792 ...\n",
      "  -1.8998810052871704 -1.8998701572418213 -1.8998589515686035]\n",
      " [-1.8999125957489014 -1.8999110460281372 -1.8999093770980835 ...\n",
      "  -1.8998948335647583 -1.8999065160751343 -1.899914264678955]\n",
      " [-1.8997936248779297 -1.8997925519943237 -1.8997913599014282 ...\n",
      "  -1.8997972011566162 -1.8997960090637207 -1.8997948169708252]]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'cube_daily_avg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-3a7af8871135>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mcube_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cube_midnight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mcube_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cube_daily_avg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'cube_daily_avg'"
     ]
    }
   ],
   "source": [
    "print cube_dict[model]['cube_midnight'][0].data\n",
    "print cube_dict[model]['cube_daily_avg'][0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubes = iris.load(directory+'tos_3hr_'+model+'_*r1i1p1f1_gn_regridded.nc')[0]\n",
    "cubes = cubes[0:10]\n",
    "#extract data for midnight\n",
    "try:\n",
    "    iris.coord_categorisation.add_hour(cubes, 'time', name='hour')\n",
    "except:\n",
    "    pass\n",
    "test = cubes[np.where(cubes.coord('hour').points == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = cubes.copy()\n",
    "\n",
    "#calculate daily mean from 3 hour snapshots\n",
    "try:\n",
    "    iris.coord_categorisation.add_day_of_year(test2, 'time', name='day')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    iris.coord_categorisation.add_year(test2, 'time', name='year')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "test2 = test2.aggregated_by(['year','day'], iris.analysis.MEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print test[0].data\n",
    "print test2[0].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate DHW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print model\n",
    "    years_over_which_to_calculate_dhw = [2012,2100]\n",
    "    mmm_climatology = mmm_skirving(cube_dict[model]['cube_midnight'])\n",
    "    dhm_cube = dhw(cube_dict[model]['cube_midnight'],mmm_climatology,years_over_which_to_calculate_dhw)\n",
    "#     dhm_cube_Spillman_2013 = dhm_Spillman_2013(cube_dict[model]['cube_midnight'],mmm_climatology,years_over_which_to_calculate_dhw)\n",
    "    lon_west = 142.0\n",
    "    lon_east = 157.0\n",
    "    lat_south = -30.0\n",
    "    lat_north = -10.0\n",
    "    dhm_cube_gbr = extract_region(dhm_cube,lon_west,lon_east,lat_south,lat_north)\n",
    "    cube_dict[model]['dhw_midnight'] = dhm_cube_gbr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print model\n",
    "    mmm_climatology = mmm_skirving(cube_dict[model]['cube_daily_avg'])\n",
    "    dhm_cube = dhw(cube_dict[model]['cube_daily_avg'],mmm_climatology,years_over_which_to_calculate_dhw)\n",
    "#     dhm_cube_Spillman_2013 = dhm_Spillman_2013(cube_dict[model]['cube_midnight'],mmm_climatology,years_over_which_to_calculate_dhw)\n",
    "    lon_west = 142.0\n",
    "    lon_east = 157.0\n",
    "    lat_south = -30.0\n",
    "    lat_north = -10.0\n",
    "    dhm_cube_gbr = extract_region(dhm_cube,lon_west,lon_east,lat_south,lat_north)\n",
    "    cube_dict[model]['dhw_daily_avg'] = dhm_cube_gbr\n",
    "#     cube_dict[model]['dhm_cube_Spillman_2013'] = dhm_cube_Spillman_2013\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print np.max(cube_dict[model]['cube_daily_avg'][200].data)\n",
    "print np.max(cube_dict[model]['cube_midnight'][200].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asb(cube,threshold):\n",
    "    dhm_cube_gbr_tmp = cube.copy()\n",
    "    dhm_cube_gbr_tmp_data = dhm_cube_gbr_tmp.data\n",
    "    dhm_cube_gbr_tmp_data[np.where(dhm_cube_gbr_tmp_data <= threshold)] = 0.0\n",
    "    dhm_cube_gbr_tmp_data[np.where(dhm_cube_gbr_tmp_data > threshold)] = 1.0\n",
    "    dhm_cube_gbr_tmp.data = dhm_cube_gbr_tmp_data\n",
    "    dhm_cube_gbr_asb = dhm_cube_gbr.copy()\n",
    "    dhm_cube_gbr_asb = dhm_cube_gbr_tmp.aggregated_by(['year'], iris.analysis.SUM)\n",
    "    dhm_cube_gbr_asb_tmp = dhm_cube_gbr_asb.data\n",
    "    dhm_cube_gbr_asb_tmp[np.where(dhm_cube_gbr_asb_tmp > 1.0)] = 1.0\n",
    "    dhm_cube_gbr_asb.data = dhm_cube_gbr_asb_tmp\n",
    "    return dhm_cube_gbr_asb\n",
    "    \n",
    "for model in models:\n",
    "    dhm_cube_gbr = extract_region(cube_dict[model]['dhw_midnight'],lon_west,lon_east,lat_south,lat_north)\n",
    "#     dhm_cube_gbr_Spillman_2013 = extract_region(cube_dict[model]['dhm_cube_Spillman_2013'],lon_west,lon_east,lat_south,lat_north)\n",
    "#     dhm_cube_gbr_tmp = dhm_cube_gbr_Spillman_2013.copy()\n",
    "    dhm_cube_gbr_asb = asb(dhm_cube_gbr,2.0)\n",
    "    dhm_cube_gbr_asb_area_avg = area_avg(dhm_cube_gbr_asb)\n",
    "    cube_dict[model]['asb_midnight'] = dhm_cube_gbr_asb\n",
    "    cube_dict[model]['asb_avg_midnight'] = dhm_cube_gbr_asb_area_avg\n",
    "    cube_dict[model]['dhw_avg_midnight'] = area_avg(dhm_cube_gbr.aggregated_by(['year'], iris.analysis.MEAN))\n",
    "    dhm_cube_gbr = extract_region(cube_dict[model]['dhw_daily_avg'],lon_west,lon_east,lat_south,lat_north)\n",
    "#     dhm_cube_gbr_Spillman_2013 = extract_region(cube_dict[model]['dhm_cube_Spillman_2013'],lon_west,lon_east,lat_south,lat_north)\n",
    "#     dhm_cube_gbr_tmp = dhm_cube_gbr_Spillman_2013.copy()\n",
    "    dhm_cube_gbr_asb = asb(dhm_cube_gbr,2.0)\n",
    "    dhm_cube_gbr_asb_area_avg = area_avg(dhm_cube_gbr_asb)\n",
    "    cube_dict[model]['asb_daily_avg'] = dhm_cube_gbr_asb\n",
    "    cube_dict[model]['asb_avg_daily_avg'] = dhm_cube_gbr_asb_area_avg\n",
    "    cube_dict[model]['dhw_avg_daily_avg'] = area_avg(dhm_cube_gbr.aggregated_by(['year'], iris.analysis.MEAN))\n",
    "#     cube_dict[model]['dhw_cube_Spillman_2013_avg'] = area_avg(dhm_cube_gbr_Spillman_2013.aggregated_by(['year'], iris.analysis.MEAN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "\n",
    "for model in models:\n",
    "#     print cube_dict[model]['asb'].coord('year').points\n",
    "#     y = cube_dict[model]['asb_avg'].data\n",
    "#     y[np.where(y > 0.0)] = 1\n",
    "    plt.scatter(cube_dict[model]['asb_daily_avg'].coord('year').points,cube_dict[model]['asb_avg_daily_avg'].data,alpha=0.5,color=model_dict[model]['color'])\n",
    "    plt.scatter(cube_dict[model]['asb_midnight'].coord('year').points,cube_dict[model]['asb_avg_midnight'].data,alpha=0.5,color=model_dict[model]['color'],marker= '+',s=250,label = model)\n",
    "\n",
    "# plt.ylim([-0.1,1.1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num=None, figsize=(8, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "# for model in models:\n",
    "#     qplt.plot(cube_dict[model]['dhm_cube_Spillman_2013_avg'],lw = 2,alpha=0.5,color=model_dict[model]['color'])\n",
    "\n",
    "    \n",
    "for model in models:\n",
    "    qplt.plot(cube_dict[model]['dhw_avg_daily_avg'],lw=2,color=model_dict[model]['color'],label = model,alpha=0.5)\n",
    "    qplt.plot(cube_dict[model]['dhw_avg_midnight'],'--',lw=2,color=model_dict[model]['color'],label = model,alpha=0.5)\n",
    "\n",
    "\n",
    "# plt.ylim([0,0.5])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note also that I'm downloading the crw sst satilite data on my desktop at moment to batmobile obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
